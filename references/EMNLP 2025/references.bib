

@book{acker1954some,
  title={Some T'ang and Pre-T'ang Texts on Chinese Painting},
  author={Acker, William Reynolds Beal},
  year={1954},
  publisher={Brill}
}

@book{panofsky1955meaning,
  title={Meaning in the Visual Arts},
  author={Panofsky, Erwin},
  year={1955},
  publisher={Doubleday}
}

@book{wolfflin1950principles,
  title={Principles of Art History: The Problem of the Development of Style in Later Art},
  author={W{\"o}lfflin, Heinrich},
  year={1950},
  publisher={Dover Publications}
}

@book{bush1971chinese,
  title={The Chinese Literati on Painting: Su Shih (1037-1101) to Tung Ch'i-ch'ang (1555-1636)},
  author={Bush, Susan},
  year={1971},
  publisher={Harvard University Press}
}

@book{siren1936chinese,
  title={The Chinese on the Art of Painting: Texts by the Painter-Critics, from the Han through the Ch'ing Dynasties},
  author={Sir{\'e}n, Osvald},
  year={1936},
  publisher={Peiping: Henri Vetch}
}


@inproceedings{chen_uniter_2020,
	address = {Cham},
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	isbn = {978-3-030-58577-8},
	abstract = {Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR\vphantom{\{}\}\vphantom{\{}\}{\textasciicircum}2\vphantom{\{}\}\vphantom{\{}\}2(Code is available at https://github.com/ChenRocks/UNITER.).},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {104--120},
}


@inproceedings{chung_style_2024,
	title = {Style {Injection} in {Diffusion}: {A} {Training}-free {Approach} for {Adapting} {Large}-scale {Diffusion} {Models} for {Style} {Transfer}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.html},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chung, Jiwoo and Hyun, Sangeek and Heo, Jae-Pil},
	month = jun,
	year = {2024},
	pages = {8795--8805},
}





@inproceedings{hong_3d-llm_2023,
	title = {{3D}-{LLM}: {Injecting} the {3D} {World} into {Large} {Language} {Models}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/413885e70482b95dcbeeddc1daf39177-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {20482--20494},
}


@article{j_peer_2024,
	title = {Peer review of {GPT}-4 technical report and systems card},
	volume = {3},
	url = {https://doi.org/10.1371/journal.pdig.0000417},
	doi = {10.1371/journal.pdig.0000417},
	number = {1},
	journal = {PLOS Digital Health},
	author = {J, Gallifant and A, Fiske and YA, Levites Strekalova and JS, Osorio-Valencia and R, Parke and R, Mwavu and N, Martinez and JW, Gichoya and M, Ghassemi and D, Demner-Fushman and R, Pierce and LA, Celi and LG, McCoy},
	month = jan,
	year = {2024},
	pages = {e0000417},
}


@inproceedings{jia_visual_2022,
	address = {Cham},
	title = {Visual {Prompt} {Tuning}},
	isbn = {978-3-031-19827-4},
	abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {709--727},
}


@inproceedings{li_blip-2_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/li23q.html},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {19730--19742},
}


@inproceedings{li_oscar_2020,
	title = {Oscar: {Object}-{Semantics} {Aligned} {Pre}-training for {Vision}-{Language} {Tasks}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision}},
	author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
	year = {2020},
	pages = {121--137},
}


@book{liu_construction_2024,
	title = {The {Construction} of {Corporate} {Identities} by {Chinese} and {American} {Airlines} on {Social} {Media}: {A} {Cross}-{Cultural} {Multimodal} {Study}},
	publisher = {Springer Nature Singapore},
	author = {Liu, Lihua},
	year = {2024},
}


@inproceedings{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {34892--34916},
}


@inproceedings{ma_vista-llama_2024,
	title = {Vista-llama: {Reducing} {Hallucination} in {Video} {Language} {Models} via {Equal} {Distance} to {Visual} {Tokens}},
	doi = {10.1109/CVPR52733.2024.01249},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ma, Fan and Jin, Xiaojie and Wang, Heng and Xian, Yuchen and Feng, Jiashi and Yang, Yi},
	year = {2024},
	pages = {13151--13160},
}


@article{messina_fine-grained_2021,
	title = {Fine-{Grained} {Visual} {Textual} {Alignment} for {Cross}-{Modal} {Retrieval} {Using} {Transformer} {Encoders}},
	volume = {17},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3451390},
	doi = {10.1145/3451390},
	abstract = {Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences (i.e., image regions and words, respectively) to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way toward the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7{\textbackslash}\% and 3.5{\textbackslash}\% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at .},
	number = {4},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
	author = {Messina, Nicola and Amato, Giuseppe and Esuli, Andrea and Falchi, Fabrizio and Gennaro, Claudio and Marchand-Maillet, Stéphane},
	month = nov,
	year = {2021},
	note = {Publisher: ACM},
	pages = {1--23},
}




@misc{meta_ai_llama_2024,
	title = {Llama 3 {Model} {Card}},
	url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md},
	author = {{Meta AI}},
	year = {2024},
}


@misc{periti_trotr_2024,
	title = {{TRoTR}: {A} {Parallel} {Corpus} of {Naturally}-{Occurring} {Text} {Reuse} for {Studying} {Recontextualization} {Effects}},
	author = {Periti, Francesco and Bang, Duhyeon and Chen, Tianzhe and Zhang, Jinglei and Rios, Miguel and Rei, Marek and Ponti, Edoardo M. and Yannakoudakis, Helen},
	year = {2024},
}


@inproceedings{periti_trotr_2024-1,
	address = {Miami, Florida, USA},
	title = {{TRoTR}: {A} {Framework} for {Evaluating} the {Re}-contextualization of {Text} {Reuse}},
	doi = {10.18653/v1/2024.emnlp-main.774},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Periti, Francesco and Cassotti, Pierluigi and Montanelli, Stefano and Tahmasebi, Nina and Schlechtweg, Dominik},
	year = {2024},
	pages = {13972--13990},
}


@incollection{qi_expressive_2024,
	address = {Singapore},
	title = {The {Expressive} {Spirit} of {Traditional} {Chinese} {Aesthetics}},
	isbn = {978-981-99-8791-7},
	url = {https://doi.org/10.1007/978-981-99-8791-7_6},
	abstract = {The term "expression" mentioned here is an abbreviated term for "expressionism," which refers to the expression of the subject's spirit. It is an opposite concept to "reproductionism," which focuses on the "reproduction" of external objects. The aesthetic spirit of ancient Chinese literature and art, which mainly focuses on lyrically expressing emotions, is the spirit of subjective expression. This is in sharp contrast to the objective reproduction spirit of Western classical literature and art, which centers on imitating external objects.},
	booktitle = {The {Spirit} of {Traditional} {Chinese} {Aesthetics}},
	publisher = {Springer Nature Singapore},
	author = {Qi, Zhixiang},
	year = {2024},
	doi = {10.1007/978-981-99-8791-7_6},
	pages = {311--323},
}


@article{qi_spirit_2024,
	title = {The {Spirit} of {Traditional} {Chinese} {Aesthetics}},
	volume = {12},
	number = {1},
	journal = {Asian Studies},
	author = {Qi, Zhixiang},
	year = {2024},
	note = {Publisher: Springer Nature Singapore},
	pages = {281--300},
}


@inproceedings{rott_shaham_multimodal_2024,
	title = {A {Multimodal} {Automated} {Interpretability} {Agent}},
	url = {https://openreview.net/forum?id=mDw42ZanmE},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Rott Shaham, Tamar and Schwettmann, Sarah and Andreassen, Anders and Goodman, Noah and Tenenbaum, Joshua and Isola, Phillip},
	year = {2024},
	pages = {24315--24325},
}




@article{salgues_self-reference_2024,
	title = {Self-reference and emotional reaction drive aesthetic judgment},
	volume = {14},
	number = {1},
	journal = {Scientific Reports},
	author = {Salgues, Sara and Jacquot, Amélie and Lassus-Sangosse, Delphine and Zagar, Daniel},
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	pages = {1--12},
}


@article{van_der_beek_invisible_2024,
	title = {The invisible divine in the history of art. {Is} {Erwin} {Panofsky} (1892–1968) still relevant for decoding {Christian} iconography?},
	volume = {11},
	number = {1},
	journal = {Journal of the Bible and its Reception},
	author = {Van der Beek, Suzanne},
	year = {2024},
	pages = {85--108},
}


@article{velcic_visual_2024,
	title = {Visual semiotics and multimodal discourse analysis},
	volume = {2024},
	number = {257},
	journal = {Semiotica},
	author = {Velčić, Vlatka},
	year = {2024},
	pages = {1--22},
}


@article{wang_changes_2024,
	title = {The {Changes} of "{Shen}" and "{Yi}" in {Chinese} {Painting} {Aesthetics}: {From} {Gu} {Kaizhi} to {Ni} {Zan}},
	volume = {11},
	url = {https://www.scirp.org/journal/paperinformation?paperid=132743},
	doi = {10.4236/oalib.1111427},
	number = {4},
	pages = {1--6},
	journal = {Open Access Library Journal},
	author = {Wang, Yuhan},
	year = {2024},
}


@inproceedings{wang_rolellm_2024,
	title = {{RoleLLM}: {Benchmarking}, {Eliciting}, and {Enhancing} {Role}-{Playing} {Abilities} of {Large} {Language} {Models}},
	booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Zhang, Man and Zhang, Zhaoxiang and Ouyang, Wanli and Xu, Ke and Huang, Wenhu Chen and Fu, Jie and Peng, Junran},
	year = {2024},
	pages = {13989--14008},
	url = {https://arxiv.org/abs/2310.00746},
}


@inproceedings{webson_prompt-based_2022,
	address = {Seattle, United States},
	title = {Do {Prompt}-{Based} {Models} {Really} {Understand} the {Meaning} of {Their} {Prompts}?},
	url = {https://aclanthology.org/2022.naacl-main.167/},
	doi = {10.18653/v1/2022.naacl-main.167},
	abstract = {Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively \{{\textbackslash}textquotedblleft\}good\{{\textbackslash}textquotedblright\} prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Webson, Albert and Pavlick, Ellie},
	year = {2022},
	pages = {2300--2344},
}


@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {24824--24837},
}


@inproceedings{wilf_think_2024-1,
	address = {Bangkok, Thailand},
	title = {Think {Twice}: {Perspective}-{Taking} {Improves} {Large} {Language} {Models}' {Theory}-of-{Mind} {Capabilities}},
	url = {https://aclanthology.org/2024.acl-long.451/},
	doi = {10.18653/v1/2024.acl-long.451},
	abstract = {Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought (CoT) have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory "Simulation Theory" to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wilf, Alex and Lee, Sihyun and Liang, Paul Pu and Morency, Louis-Philippe},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {8292--8308},
}


@article{zhang_multimodal_2024,
	title = {Multimodal {Chain}-of-{Thought} {Reasoning} in {Language} {Models}},
	volume = {2024},
	url = {https://openreview.net/forum?id=gDlsMWost9},
	journal = {Transactions on Machine Learning Research},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
	month = may,
	year = {2024},
	note = {Publisher: TMLR},
	pages = {1--35},
}


@inproceedings{zhu_minigpt-4_2024,
	title = {{MiniGPT}-4: {Enhancing} {Vision}-{Language} {Understanding} with {Advanced} {Large} {Language} {Models}},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
	year = {2024},
}


@inproceedings{ananthram2025perspective,
  title={See It from My Perspective: How Language Affects Cultural Bias in Image Understanding},
  author={Ananthram, Amith and Stengel-Eskin, Elias and Bansal, Mohit and McKeown, Kathleen},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  month={January},
  abstract={Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things. For example, individuals from Western cultures focus more on the central figure in an image while individuals from East Asian cultures attend more to scene context (Nisbett 2001). In this work, we characterize the Western bias of VLMs in image understanding and investigate the role that language plays in this disparity. We evaluate VLMs across subjective and objective visual tasks with culturally diverse images and annotations. We find that VLMs perform better on the Western split than on the East Asian split of each task. Through controlled experimentation, we trace one source of this bias in image understanding to the lack of diversity in language model construction. While inference in a language nearer to a culture can lead to reductions in bias, we show it is much more effective when that language was well-represented during text-only pre-training. Interestingly, this yields bias reductions even when prompting in English. Our work highlights the importance of richer representation of all languages in building equitable VLMs.},
  url={https://openreview.net/forum?id=Xbl6t6zxZs}
}

@article{bianchi2023language,
  title={Language Models are Multimodal Learners},
  author={Bianchi, Federico and Tsimpoukelli, Maria and Burns, Clémentine and Tan, Hao and Bardes, Adrien and Vassileios, Ballas and Pascual, Damien and Larochelle, Hugo and Courville, Aaron and Alvarez-Melis, David and Stojnic, Robert},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2023},
  url={https://openreview.net/forum?id=hygDyLr2jAv}
}

@inproceedings{wang2023evaluation,
  title={Evaluation of Large Language Models for Decision Making in Autonomous Driving},
  author={Wang, Jiageng and Guo, Shuai and Xu, Wenshuo and Zhao, Ding},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2023},
  pages={4283--4292},
  doi={10.1109/CVPRW59228.2023.00468}
}

@inproceedings{zhang2023chain,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2023},
  pages={1--14},
  url={https://aclanthology.org/2023.acl-long.147/}
}

@inproceedings{li2023blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year={2023},
  pages={20340--20355},
  url={https://proceedings.mlr.press/v202/li23q.html}
}

@inproceedings{liu2023instruction,
  title={Visual Instruction Tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277e9bbb99fc4c2d4d9dd1389ea2-Abstract-Conference.html}
}

@inproceedings{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html}
}

@inproceedings{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year={2021},
  pages={8748--8763},
  url={https://proceedings.mlr.press/v139/radford21a.html}
}

@inproceedings{ramesh2022hierarchical,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html}
}



@inproceedings{huang2023language,
  title={Language Models as Zero-Shot Reasoners},
  author={Huang, Takeshi and Kojima, Takeshi and Gu, Yusuke and Inoue, Kyohei and Iwasawa, Yutaka and Matsuo, Yutaka},
  booktitle={Proceedings of the 11th International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=e2TBb5y0yFf}
}

@inproceedings{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html}
}

@inproceedings{chen2023culturally,
  title={Culturally Appropriate Responses from Large Language Models},
  author={Chen, Alexandros and Yin, Yiyang and Peng, Nanyun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  year={2023},
  pages={11316--11331},
  url={https://aclanthology.org/2023.findings-emnlp.751}
}

@inproceedings{liu2023culturallyaware,
  title={Culturally-Aware Large Language Models},
  author={Liu, Mayank Kejriwal and Shen, Zhivar and Suri, Shreya and Tian, Yiwei and Jia, Ruoheng and Qiu, Hongting and Galstyan, Aram},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={14078--14091},
  url={https://aclanthology.org/2023.emnlp-main.868}
}

@inproceedings{nayak2024benchmarking,
  title={Benchmarking Vision Language Models for Cultural Understanding},
  author={Nayak, Shravan and Jain, Kanishk and Awal, Rabiul and Reddy, Siva and Tayyar Madabushi, Harish and Ganeriwal, Suman and Mamidi, Radhika and Kocielnik, Rafal},
  booktitle={arXiv preprint arXiv:2407.10920},
  year={2024},
  url={https://arxiv.org/abs/2407.10920}
}

@inproceedings{jiang2023personallm,
  title={PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits},
  author={Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Breazeal, Cynthia and Roy, Deb and Kabbara, Jad},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  year={2024},
  pages={3605--3627},
  url={https://arxiv.org/abs/2305.02547}
}

@inproceedings{kim2023prometheus,
  title={Prometheus: Inducing Fine-grained Evaluation Capability in Language Models},
  author={Kim, Seungone and Jang, Joel and Ye, Seonghyeon and Shin, Doyoung and Seo, Jamin and Hwang, Yongrae and Kim, Sungdong and Jang, Hyungjoo and Kim, Minjoon and Kang, Sungkyun and Kim, Minjoon and Seo, Minjoon},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={14232--14246},
  url={https://aclanthology.org/2023.emnlp-main.875}
}

@inproceedings{zhu2023multimodal,
  title={Multimodal Persona-based Generation for Situated Dialogue},
  author={Zhu, Wentao and Feng, Xiaodong and Pathak, Dhruv and Gupta, Tanmay and Singla, Karan and Hakkani-Tur, Dilek and Feizollahi, Mohammad Javad and Gao, Jianfeng and Galley, Michel},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2023},
  pages={10845--10859},
  url={https://aclanthology.org/2023.acl-long.606}
}

@inproceedings{li2023evaluating,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Zhang, Yifei and Ouyang, Kun and Gu, Yiyang and Yan, Ruiyi and Xu, Wei and Yin, Yiyang and Peng, Nanyun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={14880--14893},
  url={https://aclanthology.org/2023.emnlp-main.913}
}

@inproceedings{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhao and Lin, Yunjie and Zhao, Shuhuai and Zhang, Fangyun and Zhao, Baobao and Xie, Weizhu and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={1--10},
  url={https://arxiv.org/abs/2306.13394}
}

@inproceedings{yu2023mm,
  title={MM-React: Prompting ChatGPT for Multimodal Reasoning and Action},
  author={Yu, Zhengyuan and Cui, Linjie and Gella, Spandana and Zhou, Yuheng and Xu, Dongxu and Yang, Zhe and Gu, Jianfeng and Bansal, Mohit and Batra, Dhruv and Parikh, Devi and Strub, Florian and Feichtenhofer, Christoph and Vondrick, Carl and Shrivastava, Abhinav and Zhu, Yuke and Desai, Karan and Lan, Zhenzhong and Xu, Dejing and Wang, Xin Eric},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={1--10}
}

@inproceedings{zhang2023multimodal,
  title={Multimodal Persona-Grounded Response Generation},
  author={Zhang, Haoyu and Huang, Minlie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  year={2023},
  pages={8731--8744},
  url={https://aclanthology.org/2023.findings-acl.557}
}

@inproceedings{liu2023visual,
  title={Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
  author={Liu, Chenfei and Hu, Shengming and Zeng, Jianwei and Cui, Lingzhao and Zhao, Daxin and Jiang, Nan and Yu, Huasheng and Wen, Zhoujun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023},
  pages={1--10}
}

@inproceedings{yin2023survey,
  title={A Survey on Multimodal Large Language Models},
  author={Yin, Shukang and Xie, Chaoyou and Chen, Xinyu and Shen, Wenhai and Chen, Zhaoyang and Zeng, Zongxin and Ouyang, Wanli and Qiao, Yu and Gao, Peng},
  booktitle={arXiv preprint arXiv:2306.13549},
  year={2023},
  url={https://arxiv.org/abs/2306.13549}
}

@inproceedings{wu2023visual,
  title={Visual Instruction Tuning with Polite Flamingo},
  author={Wu, Zhiyang and Kembhavi, Aniruddha and Schwing, Alexander G. and Lazebnik, Svetlana},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/a8e8e5a8e3c9d3c8d3b6b2e9d9c8c8c8-Abstract-Conference.html}
}

@inproceedings{garcia2023visual,
  title={Visual Genome-GPT: A Vision and Language Model for Dialogue with Images},
  author={Garcia, Jing Yu Koh and Xiao, Ruslan and Tian, Yuwei and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={1--10}
}

@inproceedings{wang2023crossmodal,
  title={Cross-Modal Personalization for Multimodal Large Language Models},
  author={Wang, Zhiyang and Jiang, Ling and Yin, Xi and Ren, Xu and Soricut, Radu and Feichtenhofer, Christoph and Strub, Florian},
  booktitle={arXiv preprint arXiv:2311.04267},
  year={2023},
  url={https://arxiv.org/abs/2311.04267}
}

@inproceedings{shen2023hugginggpt,
  title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/41a60377ba920919939d83326ebee5a1-Abstract-Conference.html}
}

@inproceedings{yang2023dawn,
  title={Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
  author={Yang, Zhengyuan and Zhang, Linjie and Wang, Jianfeng and Jiang, Lu and Tan, Hao and Gella, Spandana and Bansal, Mohit and Baral, Chitta and Bisk, Yonatan and Strub, Florian and Batra, Dhruv and Parikh, Devi and Feichtenhofer, Christoph and Vondrick, Carl and Shrivastava, Abhinav and Zhu, Yuke and Desai, Karan and Lan, Zhenzhong and Xu, Dejing and Wang, Xin Eric},
  booktitle={arXiv preprint arXiv:2309.17421},
  year={2023},
  url={https://arxiv.org/abs/2309.17421}
}

@inproceedings{saberi2023art,
  title={Art Appreciation in the Era of Large Language Models: Analyzing ChatGPT's Interpretation of Visual Artworks},
  author={Saberi, Amir and Abghari, Shervin and Kvist, Malin and Nugues, Pierre},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={9876--9889},
  url={https://aclanthology.org/2023.emnlp-main.613}
}

@inproceedings{zhang2023evaluating,
  title={Evaluating Large Language Models on Art Historical Analysis},
  author={Zhang, Ruochen and Jiang, Ge and Yin, Yanzhe and Jiang, Yifan and Yin, Yiyang and Peng, Nanyun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={14894--14907},
  url={https://aclanthology.org/2023.emnlp-main.914}
}

@inproceedings{chen2023artgpt,
  title={ArtGPT-4: Artistic Vision-Language Understanding With Adapter-Enhanced MLLM},
  author={Yuan, Zhengqing and Xue, Huiwen and Wang, Xinyi and Liu, Yongming and Zhao, Zhuanzhe and Wang, Kun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={1--10}
}

@inproceedings{wang2023recontextualizing,
  title={Recontextualizing Fairness in NLP: The Case of India},
  author={Wang, Shaily and Basu, Anannya and Ghosh, Kripabandhu and Ghosh, Saptarshi},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2023},
  pages={10860--10877},
  url={https://aclanthology.org/2023.acl-long.607}
}

@inproceedings{li2023cultural,
  title={Evaluating the Cultural Awareness of Large Language Models in Understanding Idioms},
  author={Li, Yiping and Jiang, Jiayi and Yin, Yiyang and Peng, Nanyun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  pages={14908--14921},
  url={https://aclanthology.org/2023.emnlp-main.915}
}

@article{mishra_survey_2024,
  author    = {Mishra, Tanisha and Sutanto, Edward and Rossanti, Rini and Pant, Nayana and Ashraf, Anum and Raut, Akshay and Uwabareze, Germaine and Oluwatomiwa, Ajayi and Zeeshan, Bushra},
  title     = {Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers},
  journal   = {Scientific Reports},
  year      = {2024},
  volume    = {14},
  number    = {1},
  pages     = {31672},
  doi       = {10.1038/s41598-024-81370-6},
  url       = {https://doi.org/10.1038/s41598-024-81370-6},
  issn      = {2045-2322}
}

@inproceedings{NEURIPS2024_9a16935b,
 author = {Li, Cheng and Chen, Mengzhuo and Wang, Jindong and Sitaram, Sunayana and Xie, Xing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {84799--84838},
 publisher = {Curran Associates, Inc.},
 title = {CultureLLM: Incorporating Cultural Differences into Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9a16935bf54c4af233e25d998b7f4a2c-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@article{jiang_multimodal_2025,
  title={Multimodal LLMs Can Reason about Aesthetics in Zero-Shot},
  author={Jiang, Ruixiang and Chen, Changwen},
  journal={arXiv preprint arXiv:2501.09012},
  year={2025},
  url={https://arxiv.org/abs/2501.09012}
}

@article{hayashi_irr_2024,
  title={IRR: Image Review Ranking Framework for Evaluating Vision-Language Models},
  author={Hayashi, Kazuki and Onishi, Kazuma and Suzuki, Toma and Ide, Yusuke and Gobara, Seiji and Saito, Shigeki and Sakai, Yusuke and Kamigaito, Hidetaka and Hayashi, Katsuhiko and Watanabe, Taro},
  journal={arXiv preprint arXiv:2402.12121},
  year={2024},
  url={https://arxiv.org/abs/2402.12121}
}

@inproceedings{liu2024mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and Chen, Kai and Lin, Dahua},
  booktitle={Computer Vision--ECCV 2024},
  pages={216--233},
  year={2024},
  organization={Springer},
  url={https://arxiv.org/abs/2307.06281}
}

@article{yang2023harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Aiwei and Lin, Qiaoyu and Hu, Zhaoliang and Li, Shusen and Wang, Nora and Wang, Zhipeng and Zhang, Rui and Others},
  journal={arXiv preprint arXiv:2304.13712},
  year={2023}
}

@misc{zhang2023can,
      title={Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot},
      author={Zhang, Mingi and Li, Yicheng and Tian, Xufang and Zhang, Yuda and Huang, Zihan and Lu, Yuxuan and Chen, Banghua and Cao, Xueyuan and Liu, Dong},
      year={2023},
      eprint={2311.17933},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{you2023ferret,
      title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
      author={Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},
      year={2023},
      eprint={2310.07704},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2023agentverse,
      title={AgentVerse: Facilitating the Creation of LLM-Powered Multi-Agent Applications},
      author={Weize Chen and Yusheng Su and Jingwei Zuo and Cheng Yang and Chenfei Yuan and Chen Qian and Chi-Min Chan and Yujia Qin and Yaxi Lu and Ruobing Xie and Hanyu Lai and Xuhuai Liu and Mingschen Cai and Yuting Chen and Ziyun Li and Yuxiang Wu and Ganqu Cui and Weilin Zhao and Binyuan Hui and Zheyuan Zhang and Yizhou Wang and Wenhao Long and Zijun Liu and Wangchunshu Zhou and Yiming Liang and Jiahuan Pei and Chenxiao Yang and Zilu Qi and Jiatong Li and Zekai Geng and Ziyi Wang and Jinxin Wan and Yifei Yang and Wenxuan Wang and Jue KONSTA Ou and Yanbin Liu and Hong Scatt Cao and Yuxiang Zheng and Yaofu Mo and Zhengxiao Du and Xiang Long and Zhiyuan Liu and Henry B. Kautz and Jie Tang and Maosong Sun},
      year={2023},
      eprint={2308.10848},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{guo2023evaluating,
      title={Evaluating Large Language Models: A Comprehensive Survey},
      author={Zishan Guo and Renren Jin and Chuang Liu and Yufei Huang and Dan Shi and Supryadi and Linhao Yu and Yan Liu and Jiaxuan Li and Bojian Xiong and Deyi Xiong},
      year={2023},
      eprint={2310.19736},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Lin2014MicrosoftCC,
  title={Microsoft COCO: Common Objects in Context},
  author={Tsung-Yi Lin and Michael Maire and Serge J. Belongie and Lubomir D. Bourdev and Ross B. Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll{\'a}r},
  journal={ArXiv},
  year={2014},
  volume={abs/1405.0312},
  url={https://api.semanticscholar.org/CorpusID:1908COCO},
  eprint={1405.0312}
}

@misc{krishna2017visual,
  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
  year={2017},
  eprint={1602.07332},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{zhang_cultiverse_2024,
  title={CultiVerse: Towards Cross-Cultural Understanding for Paintings with Large Language Model},
  author={Zhang, Wei and Kam-Kwai, Wong and Xu, Biying and Ren, Yiwen and Li, Yuhuai and Zhu, Minfeng and Feng, Yingchaojie and Chen, Wei},
  journal={arXiv preprint arXiv:2405.00435},
  year={2024},
  url={https://arxiv.org/abs/2405.00435}
}

@article{bin_gallerygpt_2024,
  title={GalleryGPT: Analyzing Paintings with Large Multimodal Models},
  author={Bin, Yi and Shi, Wenhao and Ding, Yujuan and Hu, Zhiqiang and Wang, Zheng and Yang, Yang and Ng, See-Kiong and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2408.00491},
  year={2024},
  url={https://arxiv.org/abs/2408.00491}
}

@article{zhang_creating_2024,
  title={Creating a Lens of Chinese Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding},
  author={Zhang, Tuo and Feng, Tiantian and Ni, Yibin and Cao, Mengqin and Liu, Ruying and Butler, Katharine and Weng, Yanjun and Zhang, Mi and Narayanan, Shrikanth S. and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2406.10318},
  year={2024},
  url={https://arxiv.org/abs/2406.10318}
}

@inproceedings{reimers_sentence-bert_2019,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  pages={3982--3992},
  url={https://aclanthology.org/D19-1410/}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE},
  author={van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={86},
  pages={2579--2605},
  year={2008},
  url={https://www.jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{zhang_m3exam_2023,
  title={M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models},
  author={Zhang, Wenxuan and Aljunied, Sharifah Mahani and Gao, Chang and Chia, Yew Ken and Bing, Lidong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://arxiv.org/abs/2306.05179}
}

@inproceedings{li_seed-bench_2023,
  title={SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={13299--13308},
  url={https://arxiv.org/abs/2307.16125}
}

@article{shanahan_role_play_2023,
  title={Role play with large language models},
  author={Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  journal={Nature},
  volume={623},
  number={7987},
  pages={493--498},
  year={2023},
  publisher={Nature Publishing Group},
  url={https://www.nature.com/articles/s41586-023-06647-8}
}

% ===== Statistical Methods References (Added 2025-01-22) =====

@article{fleiss1971measuring,
  title={Measuring nominal scale agreement among many raters},
  author={Fleiss, Joseph L},
  journal={Psychological Bulletin},
  volume={76},
  number={5},
  pages={378--382},
  year={1971},
  publisher={American Psychological Association},
  doi={10.1037/h0031619}
}

@article{shrout1979intraclass,
  title={Intraclass correlations: uses in assessing rater reliability},
  author={Shrout, Patrick E and Fleiss, Joseph L},
  journal={Psychological Bulletin},
  volume={86},
  number={2},
  pages={420--428},
  year={1979},
  publisher={American Psychological Association},
  doi={10.1037/0033-2909.86.2.420}
}

@book{cohen1988statistical,
  title={Statistical Power Analysis for the Behavioral Sciences},
  author={Cohen, Jacob},
  year={1988},
  edition={2nd},
  publisher={Lawrence Erlbaum Associates},
  address={Hillsdale, NJ},
  isbn={0-8058-0283-5}
}

% ===== Technical Model References (Added 2025-01-22) =====

@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020},
  url={https://aclanthology.org/2020.acl-main.703/},
  doi={10.18653/v1/2020.acl-main.703}
}

@inproceedings{williams2018broad,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1112--1122},
  year={2018},
  url={https://aclanthology.org/N18-1101/},
  doi={10.18653/v1/N18-1101}
}

@inproceedings{xiao2023cpack,
  title={C-Pack: Packaged Resources To Advance General Chinese Embedding},
  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas and Lian, Defu and Nie, Jian-Yun},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1316--1327},
  year={2024},
  doi={10.1145/3626772.3657878},
  note={Also available as arXiv:2309.07597}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023},
  doi={10.1145/3600006.3613165},
  note={Also available as arXiv:2309.06180}
}

% ===== Art Theory and Museum Standards References (Added for dimension foundations) =====

@inproceedings{jiang2024personallm,
  title={PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits},
  author={Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Breazeal, Cynthia and Roy, Deb and Kabbara, Jad},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={3011--3030},
  year={2024},
  url={https://arxiv.org/abs/2305.02547}
}

@book{xie_he_six_canons,
  title={Guhua Pinlu [The Record of the Classification of Old Painters]},
  author={Xie, He},
  year={550},
  publisher={[Manuscript]},
  address={China},
  note={Original text circa 550 CE. English translation in: Bush, Susan and Hsio-yen Shih. Early Chinese Texts on Painting. Cambridge: Harvard University Press, 1985, pp. 39-41}
}

@book{baxandall1985patterns,
  title={Patterns of Intention: On the Historical Explanation of Pictures},
  author={Baxandall, Michael},
  year={1985},
  publisher={Yale University Press},
  address={New Haven}
}

@book{gombrich1960art,
  title={Art and Illusion: A Study in the Psychology of Pictorial Representation},
  author={Gombrich, Ernst H.},
  year={1960},
  publisher={Princeton University Press},
  address={Princeton}
}

@misc{icom_cidoc_standards,
  title={CIDOC Standards Guidelines},
  author={{International Council of Museums}},
  year={2022},
  url={https://cidoc.mini.icom.museum/standards/cidoc-standards-guidelines/},
  note={Accessed: 2024}
}

@techreport{met_museum_standards,
  title={Archives Processing and Cataloging Manual},
  author={{The Metropolitan Museum of Art}},
  year={2021},
  institution={The Metropolitan Museum of Art Archives},
  url={http://files.archivists.org/groups/museum/standards/10-MMA_Archives_Processing_and_cataloging_Manual.pdf}
}



