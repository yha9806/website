% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
%\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[preprint]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{xeCJK}
\setmainfont{Times New Roman}
%\usepackage{CJKutf8}
\usepackage{linguex}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{pifont} 
\usepackage{url}
\usepackage{subfig}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{caption}
\usepackage{todonotes}
\usepackage{paralist}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{etoolbox}
\usepackage{breqn}
\usepackage{amsfonts}


\title{A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and... \and Author n \\
%         Address line \\... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\ ... \\ Address line
%         \And ... \And
%         Author n \\ Address line \\... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\ ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\... \\ Address line \And
%         Author 3 \\ Address line \\... \\ Address line}

%\author{Haorui Yu \\
%  DJCAD, \\
%  University of Dundee, \\
%  United Kingdom \\
%  \texttt{2655435@dundee.ac.uk} \\\And
%  Ramon Ruiz-Dolz \\
%  Centre for Argument Technology, \\
%  University of Dundee, \\
%  United Kingdom \\
%  \texttt{rruizdolz001@dundee.ac.uk} \\\AND
%  Qiufeng Yi \\ %\footnote{Corresponding author.} \\
%  School of Computer Science, \\
%  University of Birmingham, \\
%  United Kingdom \\
%  \texttt{q.yi@bham.ac.uk}}

\author{
  \textbf{Haorui Yu\textsuperscript{1}},
  \textbf{Ramon Ruiz-Dolz\textsuperscript{2}}, and
  \textbf{Qiufeng Yi\textsuperscript{3}}
\\
  \textsuperscript{1}DJCAD, University of Dundee, United Kingdom \\
  \textsuperscript{2}Centre for Argument Technology, SSEN, University of Dundee, United Kingdom \\
  \textsuperscript{3}School of Computer Science, University of Birmingham, United Kingdom\\
  \texttt{\{2655435, rruizdolz001\}@dundee.ac.uk} \\
  \texttt{q.yi@bham.ac.uk}
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}

\maketitle
%\begin{CJK*}{UTF8}{gbsn}

% Input figures and tables definitions early to ensure labels are known before reference


\begin{abstract}
This study aims to test and evaluate the capabilities and characteristics of current mainstream Visual Language Models (VLMs) in generating critiques for traditional Chinese painting. To achieve this, we first developed a quantitative framework for Chinese painting critique. This framework was constructed by extracting multi-dimensional evaluative features covering evaluative stance, feature focus, and commentary quality from human expert critiques using a zero-shot classification model. Based on these features, several representative critic personas were defined and quantified. This framework was then employed to evaluate selected VLMs such as Llama, Qwen, or Gemini. The experimental design involved persona-guided prompting to assess the VLM's ability to generate critiques from diverse perspectives. Our findings reveal the current performance levels, strengths, and areas for improvement of VLMs in the domain of art critique, offering insights into their potential and limitations in complex semantic understanding and content generation tasks. The code used for our experiments can be publicly accessed at: \url{https://github.com/yha9806/EMNLP2025-VULCA}.
\end{abstract}

\section{Introduction}

%\textit{"It is especially hard to find objective metrics that distinguish between persuasive bullshit and authentic excellence in matters such as morality and aesthetics." -- Matt Prewitt, 2024}

Large language models (LLMs) have demonstrated remarkable performance on general NLP benchmarks, yet their applicability in culturally embedded, humanistic domains remains limited. In high-context interpretive tasks such as art criticism, clinical narrative analysis, or historical commentary, model performance depends not only on linguistic fluency or factual accuracy, but also on deeper forms of cognitive alignment—epistemic sensitivity, rhetorical coherence, and cultural adaptability.

A representative and particularly demanding testbed for such capabilities is \textit{Chinese art commentary}. This genre, especially when analyzing works like traditional landscape or court paintings, involves symbolic interpretation, aesthetic judgment, and deeply situated cultural discourse. Existing multimodal LLMs are rarely evaluated in this space. Standard benchmarks such as MME~\cite{fu2023mme} and MMBench~\cite{liu2024mmbench} focus on object recognition or task-oriented vision-language reasoning, while frameworks like ArtGPT~\cite{chen2023artgpt} emphasize captioning and factual grounding. These methods largely overlook interpretive nuance and disciplinary diversity.

Meanwhile, humanistic commentary often exhibits non-linear logic, specialized lexicons, and varied stylistic conventions, particularly in Chinese art contexts where rhetorical strategies such as \textit{yijing} (意境, artistic conception) or \textit{qiyun shengdong} (气韵生动, spiritual resonance) are essential but difficult to quantify~\cite{bush1971chinese, siren1936chinese}. Without appropriate grounding, LLMs risk producing synthetic outputs that mimic surface patterns but fail to demonstrate epistemic alignment~\cite{guo2023evaluating}. This growing mismatch calls for new paradigms in evaluation and adaptation.

%\vspace{0.5em}
To address these challenges, we introduce VULCA—the \textit{Vision-Understanding and Language-based Cultural Adaptability Framework}. VULCA is a structured evaluation and enhancement framework designed to assess how well VLMs align with domain-specific interpretive practices in culturally situated tasks. Our work centers on Chinese art commentary, but the methodology generalizes to other multimodal and epistemically rich domains such as religion, medicine, or history. VULCA combines three core components: (1) a multi-dimensional human expert benchmark (MHEB) constructed from 163 art commentaries annotated across five cultural capability dimensions; (2) a persona-guided recontextualization mechanism using eight interpretive personas and a domain-specific knowledge base; and (3) a joint evaluation pipeline integrating vector-space semantic alignment with rubric-based capability scoring. Commentaries are generated from annotated traditional Chinese paintings, and their alignment with expert patterns is evaluated with and without interventions. As a result, we produce five contributions: (i) the definition of VULCA, a new structured framework for assessing and enhancing VLMs in culturally grounded, multimodal reasoning tasks; (ii) we construct MHEB, a high-quality human benchmark of Chinese art commentary annotated across five capability dimensions; (iii) we develop and evaluate persona-guided recontextualization interventions using eight expert personas and a domain-specific knowledge base; (iv) we demonstrate over 20\% improvement in symbolic reasoning and over 30\% improvement in argumentative coherence on Gemini 2.5 Pro using our proposed method; and (v) we establish the generalizability of our evaluation methodology to other epistemically rich domains such as religion, history, and education.

%\vspace{0.5em}
%\textbf{This paper investigates the following five research questions:}
%\begin{itemize}
%    \item[\textbf{RQ1}] How closely do baseline VLM commentaries align with expert human outputs in semantic space and stylistic form?
%    \item[\textbf{RQ2}] How effective are persona-guided prompts and knowledge grounding in enhancing cultural reasoning?
%    \item[\textbf{RQ3}] Which interpretive capabilities—such as symbolic reading, evaluative stance, or argumentative quality—show the greatest improvement under intervention?
%    \item[\textbf{RQ4}] How do different VLMs vary in responsiveness to structured recontextualization?
%    \item[\textbf{RQ5}] Can the proposed evaluation framework generalize to other interdisciplinary domains beyond Chinese art?
%\end{itemize}

%\vspace{0.5em}
%\textbf{This paper makes the following five contributions:}
%\begin{itemize}
%    \item We introduce VULCA, a structured framework for assessing and enhancing VLMs in culturally grounded, multimodal reasoning tasks.
%    \item We construct D1, a high-quality human benchmark of Chinese art commentary annotated across five capability dimensions.
%    \item We develop and evaluate persona-guided recontextualization interventions using eight expert personas and a domain-specific knowledge base.
%    \item We demonstrate over 20\% improvement in symbolic reasoning and over 30\% improvement in argumentative coherence on Gemini 2.5 Pro using our method.
%    \item We establish the generalizability of our evaluation methodology to other epistemically rich domains such as religion, history, and education.
%\end{itemize}

Together, our work highlights the need for new evaluation paradigms that go beyond benchmark metrics and toward measuring how well LLMs can adapt to the interpretive demands of real-world, interdisciplinary contexts.



\section{Related Work}

\paragraph{Missing Evaluation Dimensions for Cultural Reasoning.}
Despite significant advances in multimodal evaluation, current benchmarks primarily target factual understanding rather than cultural interpretation. Existing benchmarks for large or multimodal language models, such as \cite{fu2023mme, you2023ferret}, emphasize factual accuracy or instruction following, seldom addressing symbolic interpretation or epistemic alignment. Recent cultural evaluation efforts like M3Exam \cite{zhang_m3exam_2023} and SEED-Bench \cite{li_seed-bench_2023} begin to incorporate cultural knowledge but focus on factual recall rather than interpretive reasoning. ArtGPT \cite{chen2023artgpt}, for instance, evaluates stylistic generation but lacks formal metrics for interpretive depth. While prior work explores aesthetic reasoning \cite{wang_changes_2024}, these studies rarely offer structured, multi-capability evaluation. Our work addresses this gap by introducing cultural adaptability, operationalized through a multi-dimensional human expert benchmark with capability rubrics, enabling quantitative comparison in high-context domains like Chinese art.

\paragraph{Limitations of Persona Conditioning Without Grounding.}
Building on evaluation gaps, current persona-based approaches show promise but remain limited in cultural domains. Persona use in LLM evaluation shows promise for style control \cite{jiang2024personallm, wang_rolellm_2024}, yet most methods lack structured knowledge grounding, especially in epistemically rich domains. While recent work on role-playing \cite{shanahan_role_play_2023} and character conditioning demonstrates behavioral adaptation, these approaches often rely on surface-level stylistic changes rather than deep domain expertise. Our method addresses this limitation by combining persona simulation with curated domain-specific knowledge to guide generation towards symbolic reasoning and cultural interpretation, not just stylistic alignment, offering a controlled intervention mechanism.

\paragraph{Gap in Multimodal Input–Interpretation Evaluation.}
Current multimodal frameworks like MMBench or LLaVA \cite{liu_visual_2023} primarily focus on classification, question answering, or instruction following, rarely requiring grounded interpretation. Our pipeline links annotated symbolic elements with structured prompts for art commentary, evaluating VLM outputs for semantic alignment with MHEB using vector-space and rubric-based metrics, addressing a gap in assessing image-conditioned cultural reasoning.

\paragraph{Lack of Comparative Cultural Interventions Across Models.}
Surveys \cite{guo2023evaluating} discuss LLM limitations in nuanced discourse, but few studies compare model responsiveness to structured cultural interventions. Our empirical evaluation shows persona and knowledge base intervention improves symbolic reasoning and argumentative coherence by over 20–30\%, highlighting epistemic alignment's role beyond fluency. This cross-model, capability-specific analysis distinguishes our work.

\section{Methodology}

\begin{figure}[htbp] % 使用 figure 使其在单栏显示，htbp 尝试最佳位置
    \centering
    \includegraphics[width=\linewidth, keepaspectratio]{picture/paper_structure.png} % 引用框架图路径，宽度改为linewidth
    \caption{Overview of the VULCA framework, illustrating its components and their interactions for structured evaluation and intervention in art criticism.}
    \label{fig:vulca_framework}
\end{figure}



This research aims to comprehensively evaluate Visual Language Models (VLMs) capabilities in generating critiques for traditional Chinese painting, assessing their understanding of image content, commentary quality, and adaptability to guided perspectives. The workflow involves: Framework Construction, developing a quantitative analytical framework from human expert commentaries, including defining evaluative dimensions and critic personas; VLM evaluation experiment design, creating structured protocols for VLM critique generation under conditions like persona-based and baseline prompting; and experimentation and result analysis, implementing experiments, collecting VLM critiques, and analyzing them with the developed framework to assess capabilities and intervention impacts. Figure~\ref{fig:vulca_framework} provides an overview of this framework and its components.

A cornerstone is the quantitative framework benchmark for VLM critiques, built upon human expert commentaries on Chinese art. To ensure objective, reproducible, and fine-grained evaluation, an automated capability assessment framework was developed. This involves feature extraction, multi-dimensional capability scoring, profile assignment, and visualization, using a zero-shot classification model for fine-grained evaluative labels. The scoring covers painting element recognition, Chinese painting understanding, and language usage, each with a dedicated rubric. This structured, rule-based approach enhances objectivity and facilitates large-scale benchmarking \cite{jiang_multimodal_2025, hayashi_irr_2024}.

\subsection{MHEB Construction and Annotation Process}
\label{sec:mheb-construction}

Our three-dimensional evaluation framework synthesizes Eastern and Western art criticism traditions with modern museum documentation standards into the three major dimensions of Evaluative Stance, Feature Focus, and Commentary Quality. The framework draws from:

(1) \textit{Chinese Art Theory}: Building on Xie He's Six Canons (六法, 550 CE)~\cite{xie_he_six_canons}, particularly the concepts of ``spirit resonance'' (气韵生动) and ``bone method'' (骨法用笔), which inform our Feature Focus dimension's emphasis on brushwork technique, artistic conception, and emotional expression.

(2) \textit{Western Art Historical Methods}: Incorporating Baxandall's ``inferential criticism''~\cite{baxandall1985patterns} and Gombrich's psychological approach~\cite{gombrich1960art}, which contribute to our Evaluative Stance dimension through categories like comparative analysis, theoretical construction, and critical inquiry.

(3) \textit{Museum Documentation Standards}: Following international cataloging frameworks from ICOM-CIDOC~\cite{icom_cidoc_standards} and practices from the Palace Museum Beijing, National Palace Museum Taipei, and Metropolitan Museum of Art~\cite{met_museum_standards}, which standardize descriptive categories for artwork documentation. These inform our systematic approach to feature extraction and the structured nature of our Commentary Quality dimension.

This synthesis creates a culturally-informed yet methodologically rigorous framework that captures both the technical aspects emphasized in Western criticism (e.g., composition, color theory) and the philosophical-spiritual dimensions central to Chinese art evaluation (e.g., artistic conception, symbolic meaning). %The 47 feature labels across three dimensions were iteratively refined through pilot annotations on a subset of expert commentaries to ensure comprehensive coverage of critical discourse patterns in Chinese art criticism.
The MHEB was therefore systematically constructed through the following process:

\textbf{Data Collection.} We collected 163 expert commentaries from authoritative sources including museum catalogs from the Palace Museum Beijing, National Palace Museum Taipei, and Metropolitan Museum of Art, as well as peer-reviewed art history journals and monographs by recognized scholars specializing in Qing court painting. Each commentary averages 500-800 Chinese characters and provides in-depth analysis of specific paintings from the ``Twelve Months'' series. The annotation process generated 558 total annotation instances (163 texts × 3 annotators plus quality control samples), which were consolidated into 163 final records after resolving disagreements.

\textbf{Expert Sources.} The 163 commentaries in MHEB were extracted from scholarly publications by 9 distinguished art historians specializing in Chinese painting and Qing court art. The corpus includes: Xue Yongnian (薛永年, 17 texts from two monographs), Wang Di (汪涤, 28 texts), Yang Danxia (杨丹霞, 28 texts), Nie Chongzheng (聂崇正, 15 texts), Shan Guoqiang (单国强, 18 texts), Li Shi (李湜, 17 texts), Xu Jianrong (徐建融, 17 texts), Zhu Wanzhang (朱万章, 11 texts), and Chen Yunru (陈韵如, 12 texts). These experts represent major institutions including the Palace Museum Beijing, National Palace Museum Taipei, and leading Chinese art history departments, ensuring diverse yet authoritative perspectives on Giuseppe Castiglione's ``Twelve Months'' series.

\textbf{Annotation Process.} Three annotators with graduate-level training in Chinese art history independently labeled each commentary. Annotators were provided with a 20-page annotation guideline detailing the three evaluation dimensions (Evaluative Stance, Feature Focus, Commentary Quality) and their respective sub-categories. Each annotator spent approximately 15-20 minutes per commentary, assigning scores for all 38 primary feature labels using a 0-1 continuous scale based on presence and prominence, from which 9 additional analytical dimensions were derived. Annotation was performed independently using a custom web-based interface, with randomized presentation order to minimize bias.

\textbf{Quality Control Measures.} To ensure annotation quality throughout the process, we implemented multiple control mechanisms: (1) 20\% of commentaries were double-annotated to monitor consistency; (2) bi-weekly calibration sessions were held over the 3-month annotation period where annotators discussed challenging cases and aligned their understanding; (3) continuous monitoring tracked annotator performance and drift. These measures ensured that the annotation process remained consistent and reliable throughout the data collection period.

\textbf{Inter-Annotator Agreement (IAA).} To quantitatively assess the reliability of our annotations, we calculated inter-annotator agreement using two complementary metrics. For categorical labels (e.g., stance categories), we computed Fleiss' kappa~\cite{fleiss1971measuring}, which measures agreement beyond chance for multiple raters. For continuous scores (e.g., feature prominence ratings from 0-1), we calculated the intraclass correlation coefficient (ICC)~\cite{shrout1979intraclass}, which assesses the consistency of quantitative measurements across raters. The average Fleiss' kappa across stance categories was 0.78, indicating substantial agreement according to Landis and Koch's interpretation scale. The ICC for feature prominence scores reached 0.82, demonstrating excellent reliability. When disagreements occurred (defined as κ < 0.6 for specific labels), they were resolved through discussion, with a senior art historian serving as arbiter for persistent conflicts. The stable inter-rater agreement (κ variation < 0.05 across time) validated the effectiveness of our quality control measures. Final dataset statistics show balanced representation across different evaluative stances (Historical: 31\%, Aesthetic: 28\%, Technical: 23\%, Comparative: 18\%) and comprehensive coverage of feature focus.



\subsection{Feature Engineering from Human Expert Critiques}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth, keepaspectratio]{picture/combined_author_semantic_space_visualization_tsne(作者语义空间t-SNE可视化).png}
    \caption{T-SNE visual representation of human expert art commentaries.}
    \label{fig:semantic-visual}
\end{figure*}

Framework foundation relies on human expert commentaries, significantly from Giuseppe Castiglione's (Lang Shining) ``Twelve Months'' (十二月令图) series—Qing imperial court paintings fusing Chinese and Western traditions. To enhance model training and evaluation, a sliding window cropping strategy (640×640 pixel sub-images) was applied to these high-resolution images, augmenting data diversity and granularity for improved VLM detail recognition and evaluation accuracy, a common practice in computer vision (e.g., \cite{Lin2014MicrosoftCC, krishna2017visual}).

We employed a zero-shot classification model to systematically extract evaluative characteristics. Specifically, we used the multilingual BART-large-mnli model~\cite{lewis2020bart, williams2018broad}, which has been fine-tuned on natural language inference tasks and can classify text into arbitrary categories without task-specific training. For each commentary text, the model computes the probability of belonging to each predefined label using the entailment paradigm. Given a text $T$ and a label $L$, the model evaluates the hypothesis ``This text is about $L$'' and outputs a softmax probability score $p(L|T) \in [0,1]$. We apply this process across 38 labels spanning three dimensions: \textit{Evaluative Stance} (10 labels, e.g., ``Historical Research'', $p=0.85$), \textit{Feature Focus} (17 labels, e.g., ``Use of Color'', $p=0.72$), and \textit{Commentary Quality} (11 labels, e.g., ``Profound Insight'', $p=0.68$). Furthermore, we complemented this set of 38 labels with 9 additional labels representing higher level features: 5 profile alignment scores derived from clustering analysis of the 38 primary features, and 4 supplementary analytical dimensions for enhanced discrimination between critique styles.

Thresholds for binary classification were empirically determined through validation on a held-out subset: labels with $p > 0.5$ are considered present, while prominence levels are captured by the continuous scores. This comprehensive 47-dimensional feature vector (38 primary features plus 9 derived dimensions) enables nuanced quantitative comparison and clustering. Appendix~\ref{app:zeroshot_labels} provides complete list of all 47 dimensions: the 38 primary labels and 9 derived analytical dimensions. Figure~\ref{fig:semantic-visual} visualizes the MHEB semantic distribution from these features.

The zero-shot classification model serves as an analytical tool for deconstructing expert texts and building our evaluation framework, distinct from the VLMs (e.g., Gemini 2.5 Pro, Qwen-VL) evaluated later.



\subsection{Evaluation of Dimensions and Label System}
%To systematically analyze commentary content, we define a structured annotation scheme based on three major dimensions: Stance, Feature Focus, and Commentary Quality. 

The three dimensions of our framework (i.e., Evaluative Stance, Feature Focus, and Commentary Quality) were derived from multiple sources: (1) traditional Chinese painting theory, particularly Xie He's ``Six Principles of Painting'' (谢赫六法)~\cite{acker1954some} which emphasizes spirit resonance (气韵生动), bone method (骨法用笔), and correspondence to nature (应物象形); (2) Western art criticism frameworks from Panofsky's three levels of meaning~\cite{panofsky1955meaning} and Wölfflin's formal analysis principles~\cite{wolfflin1950principles}; (3) consultations with curators from the Palace Museum and Metropolitan Museum who validated the relevance of these dimensions for Qing court painting analysis; and (4) empirical analysis of recurring patterns in our collected expert commentaries. %These dimensions were refined through pilot studies with expert annotations.

Evaluative Stance characterizes the rhetorical or evaluative position taken by the commentator (e.g., historical interpretation, praise, or critique). Feature Focus identifies the specific visual or contextual aspects discussed in the commentary (e.g., line quality, symbolism, spatial composition). Commentary Quality captures the analytical depth and logical structure of the commentary, ranging from clear, well-argued insights to superficial or biased remarks. Furthermore, each dimension comprises a set of fine-grained subcategories with bilingual English–Chinese mappings. Full definitions and label lists are provided in Appendix~\ref{app:label_definitions}.

%To illustrate the system, we briefly explain one representative label from each dimension:

%\textbf{Stance – Aesthetic Appreciation (美学鉴赏型):} Commentary focuses on the beauty and expressive power of the painting, often using evocative or poetic language to highlight visual elegance or emotional resonance. \textbf{Feature Focus – Brushwork Technique (笔法技巧):} The analysis emphasizes the artist's brushstroke styles, control, or variation, such as fine lines, dry brush texture, or fluid ink application. \textbf{Commentary Quality – Profound Insight (见解深刻独到):} The argument demonstrates deep understanding, originality, and relevance, going beyond surface observations to offer meaningful interpretations.

%These representative sub-dimensions help bridge formal annotation with art historical reasoning. The full taxonomy serves as the foundation for profile construction, persona classification, and performance evaluation.

\subsection{Construction and Definition of Critic Personas}
To capture holistic critique style and depth beyond granular features, we constructed ``critic personas'' representing archetypal critical perspectives. Their development was data-driven, analyzing features from human expert commentaries, complemented by art history domain expertise. Five core personas were defined: Comprehensive Analyst (博学通论型), Historically Focused Critic (历史考据型), Technique \& Style Focused Critic (技艺风格型), Theory \& Comparison Focused Critic (理论比较型), and General Descriptive Profile (泛化描述型). These five core personas represent data-driven evaluation categories derived from clustering analysis of human expert features, serving as benchmarks for assessing whether VLM outputs align with recognizable expert critique patterns.

Each persona is quantitatively defined by rules and thresholds based on zero-shot classification feature scores. This rule-based matching objectively assigns commentaries (human or VLM) to personas. Persona definition and matching rely on explicit features and rule-based logic, not primarily direct semantic embedding of raw text. Dimensionality reduction (t-SNE/UMAP) visualizes commentary and persona distribution in the feature space, not for initial persona vector generation. 

%\subsection{Value and Application of the Framework}
%The resulting quantitative framework, which integrates fine-grained feature analysis with the abstracted critic personas, offers a multi-dimensional, quantifiable, and empirically grounded benchmark. Rooted in the discernible characteristics of human expert critiques, this framework provides a structured and robust foundation for the subsequent systematic evaluation and comparative analysis of Chinese painting commentaries generated by Visual Language Models.

\subsection{Task Definition}
\label{sec:exp-protocol} % Added label for sec:exp-protocol

This quantitative framework guided experiments evaluating selected VLMs (e.g., Gemini 2.5 Pro, Qwen-VL). The core task required VLMs to generate commentary on provided traditional Chinese painting images. Experiments typically involved structured, multi-round interactions for each VLM per image, including persona-based and baseline Q\&A rounds.

Inputs were multifaceted: high-definition ``Monthly Images'' (sometimes segmented); predefined ``Persona Cards'' \cite{jiang2024personallm} serving as experimental interventions-distinct from the five evaluaction personas above, these eight cultural perspective prompts VLM generation: guiding analysis—Mama Zola (佐拉妈妈), Professor Elena Petrova (埃琳娜·佩特洛娃教授), Okakura Kakuzō (冈仓天心), Brother Thomas (托马斯修士), John Ruskin (约翰·罗斯金), Su Shi (苏轼), Guo Xi (郭熙), and Dr. Aris Thorne (阿里斯·索恩博士); standardized prompt templates \cite{nayak2024benchmarking}; and an optional JSON knowledge base \cite{zhang_cultiverse_2024, bin_gallerygpt_2024}. Persona guidance aimed to assess VLM capability to simulate diverse perspectives and analytical styles \cite{zhang_creating_2024}. See Appendix \ref{app:personas} for a detailed summary of each critic persona included in our study. To avoid confusion, we distinguish between the use of personas at two different levels: the five core personas described in the previous sub-section are data-driven evaluation categories for classifying generated critiques based on feature patterns, while the eight persona cards are cultural perspective prompts used to guide VLM generation during experiments. The former evaluates outputs, while the latter shapes inputs.

%All VLM-generated texts were recorded and systematically organized. These outputs were then analyzed using the quantitative framework (Section 3.2), applying zero-shot classification to extract feature scores and matching critiques against predefined "Critic Personas" to assess alignment, especially under specific persona guidance.

The VLM critique evaluation dimensions cover: \textit{Painting Element Recognition} (5-point scale); \textit{Chinese Painting Understanding} (7-point scale); and \textit{Chinese Language Usage} (5-point scale). Prompt design, particularly for structured commentary, targeted these dimensions.


\subsection{Vector Space Representation and Visualization}
To compare human and VLM critiques, we converted feature scores (Evaluative Stance, Feature Focus, Commentary Quality) from both into numerical vectors. These vectors were projected into a 2D space using t-SNE for visualisation \cite{van2008visualizing}, enabling assessment of semantic similarity and distributional differences. Figure~\ref{fig:persona_impact_composite} (left) illustrates such a comparative visualization, showing the semantic distribution of human expert commentaries versus baseline VLM-generated commentaries, highlighting their initial semantic gap. 

%These visualizations help analyze how VLM outputs align with human expert benchmarks, identify specific VLM strengths/weaknesses, and assess persona/knowledge interventions' impact on aligning VLM critiques with desired expert profiles.

%\subsection{Automated Workflow}
%\label{sub:automated_workflow_main} % Adding a label for potential cross-ref

%This research implemented a modular, automated experimental pipeline for profile scoring, dimensionality reduction, and dataset preparation for visualizations.

%Experimental benchmarking involved VLM commentary generation using a curated artwork dataset with varied prompts (baseline, persona-specific, knowledge-enhanced). VLM outputs were logged, versioned, and organized by model, persona, and prompt. Subsequent automated analysis involved feature extraction, persona scoring, and comparative metrics generation. This systematic approach facilitated large-scale, reproducible evaluation of VLM performance in Chinese art critique.

\subsection{Multi-Model Comparative Evaluation}

To comprehensively assess the capabilities of state-of-the-art large language and vision-language models, we conducted a systematic comparative evaluation across four representative models: Google Gemini 2.5 Pro, Meta Llama-3.1-8B-Instruct, Meta Llama-4-Scout-17B-16E-Instruct, and Qwen-2.5-VL-7B. All models were evaluated using the same experimental protocol, dataset splits, and evaluation metrics to ensure fair and reproducible comparison. 

%All models were accessed via their official APIs or open-source checkpoints, with inference settings  kept consistent. For multimodal tasks, only models supporting both text and image inputs were included in the corresponding benchmarks. The evaluation covers a range of tasks, including argumentative quality, core focal points, stance analysis, and semantic space visualization, as detailed in Section~\ref{app:evaluation}.

\subsection{Quantitative Modeling and Formalisms}
\label{sec:quantitative_formalisms}

This section details the key mathematical formulations used in our analytical framework, covering semantic representation, comparative metrics, and the profile matching algorithm.

\paragraph{Semantic Embedding.}
Conceptually:
\begin{equation}
\mathbf{v}_d = \text{SentenceTransformer}(\text{document}_d)
\end{equation}
Where ($\mathbf{v}_d \in \mathbb{R}^{N}$) (e.g., ($N=1024$) for BAAI/bge-large-zh-v1.5~\cite{xiao2023cpack}).

\paragraph{Average Quality Score for Radar Chart (\(\bar{q}_{j,G}\)).}
For a quality dimension \(j\) and a group of documents \(G\) (e.g., Human Experts, VLM Baseline):
\begin{equation}
\bar{q}_{j,G} = \frac{1}{|N_G|} \sum_{d \in N_G} s_{j,d}
\end{equation}
Where \(s_{j,d}\) is the score of document \(d\) on quality dimension \(j\), and \(|N_G|\) is the number of documents in group \(G\).

\paragraph{Centroid Calculation in Dimensionality Reduced Space (\(\mathbf{c}_p\)).}
For a profile/condition \(p\), its centroid in a 2D space (e.g., t-SNE):
\begin{equation}
\mathbf{c}_p = (\bar{x}_p, \bar{y}_p) = \left( \frac{1}{|D_p|} \sum_{d \in D_p} x_d, \frac{1}{|D_p|} \sum_{d \in D_p} y_d \right)
\end{equation}
Where \((x_d, y_d)\) are the 2D coordinates of document \(d\) belonging to profile/condition \(p\), and \(|D_p|\) is the number of documents in profile/condition \(p\).

\paragraph{Cohen's d (Effect Size)~\cite{cohen1988statistical}.}
To measure the standardized difference between two group means (\(\bar{X}_1, \bar{X}_2\)):
\begin{equation}
d = \frac{\bar{X}_1 - \bar{X}_2}{s_p}
\end{equation}
Where \(s_p\) is the pooled standard deviation:
\begin{equation}
s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}
\end{equation}
And here \(n_1, n_2\) are the sample sizes of group 1 and group 2, while \(s_1^2, s_2^2\) are the variances of group 1 and group 2.

\paragraph{Stance Contribution Formula ($S_C$).}
We compute the stance contribution $S_C$ using the following conditions:
\begin{dmath*}
S_C = 
\begin{cases}
\frac{s_{\text{actual}} - s_{\text{min\_rule}}}{s_{\text{max\_rule}} - s_{\text{min\_rule}}}, 
& \text{if } \begin{aligned}[t] & L_{\text{actual}} = L_{\text{rule}}, \\
& s_{\text{actual}} \geq s_{\text{min\_rule}}, \\
& s_{\text{max\_rule}} \neq s_{\text{min\_rule}} \end{aligned} \\
1, & \text{if } \begin{aligned}[t] & L_{\text{actual}} = L_{\text{rule}}, \\
& s_{\text{actual}} \geq s_{\text{min\_rule}}, \\
& s_{\text{max\_rule}} = s_{\text{min\_rule}} \end{aligned} \\
0, & \text{otherwise}
\end{cases}
\end{dmath*}
Where $S_C$ is the stance contribution score, $L_{\text{actual}}$ is the actual stance label of the text, $L_{\text{rule}}$ is the required stance label in the profile rule, $s_{\text{actual}}$ is the actual stance score, and $s_{\text{min\_rule}}$, $s_{\text{max\_rule}}$ represent the required range.

% 在这里直接包含composite_figure_tsne_radar.png图表，将其放在第一个小节之前
\begin{figure*}[!ht] 
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{picture/composite_figure_tsne_radar.png}
    \caption{Impact of Persona and Knowledge Base Interventions on VLM Critiques: A comprehensive analysis comparing intervened VLM outputs with a human expert benchmark. Left: t-SNE and KDE plots visualize the semantic distribution of critiques from different sources (human experts, baseline VLMs, intervened VLMs). Right: A radar chart compares average capability scores across dimensions like Profound Insight and Logical Clarity.}
    \label{fig:persona_impact_composite}
\end{figure*}

\section{Results}
\label{sec:results}

We present our results from semantic alignment, capability profiling, and the effects of persona-guided interventions on VLMs. All evaluations are made with respect to the MHEB, using both vector-space analysis and rubric-based scoring.

\subsection{Semantic Divergence from Expert Commentary}

Baseline VLM outputs exhibit significant divergence from human expert commentaries. As shown in Figure~\ref{fig:persona_impact_composite} (left), expert texts cluster tightly in semantic space, while VLM outputs are more dispersed and form distinct clusters. Profile-based visualizations (Figure~\ref{fig:profiling_summary_comparison} (right)) further confirm this divergence: baseline models frequently align with generic or technique-oriented profiles, rarely matching complex expert personas.

% 在这里直接包含profiling_summary_figure.png图表，将其放在相关小节之后
\begin{figure*}[!ht] 
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{picture/profiling_summary_figure.png}
    \caption{Profiling Summary: A comparative visualization of Human Experts vs. VLMs across key textual features (left), mean profile alignment scores (center), and t-SNE projection of profile vectors (right).}
    \label{fig:profiling_summary_comparison}
\end{figure*}

\subsection{Capability Profile Differences}

Human expert commentaries, as quantified by our ZSL analysis (see Table~\ref{tab:key_feature_scores} in Appendix~\ref{app:quantitative_tables} for full data which Figure~\ref{fig:profiling_summary_comparison} (left) visualizes), emphasize symbolic and historical interpretation (e.g., average scores of 0.676 in Historical Context and 0.661 in Symbolism) but notably less on technical aspects like Brushwork Technique (0.199). They also exhibit high subjectivity and non-linear reasoning (e.g., 0.674 in Subjective View, 0.093 in Clear Logic, as detailed in Table~\ref{tab:radar_chart_capability_scores}). 

In contrast, baseline VLMs show varied performance. For instance, Llama-4-Scout-17B-16E-Instruct achieves high scores in Historical Context (0.710) and Symbolism (0.758), comparable to or exceeding human experts. Qwen-2.5-VL-7B also performs well in these areas (0.650 and 0.773 respectively) and particularly excels in Artistic Conception (0.891) and Brushwork Technique (0.937), the latter being dramatically higher than the human expert average of 0.199 for this feature (see Table~\ref{tab:key_feature_scores}). Gemini-2.5pro shows strength in Layout and Structure (0.874), while Meta-Llama-3.1-8B-Instruct generally presents lower scores across several nuanced dimensions like Historical Context (0.366) and Symbolism (0.529). These differences are summarized in Figure~\ref{fig:profiling_summary_comparison} (left) and supported by the radar plots in Figure~\ref{fig:persona_impact_composite} (right).

\subsection{Effectiveness of Persona-Guided Interventions}

Persona-guided prompting, especially when supported by domain knowledge, substantially improves VLM outputs. Figure~\ref{fig:persona_impact_composite} (right) illustrates that Qwen-2.5-VL improves scores across key dimensions---e.g., Profound Insight (from 0.31 to 0.61), Strong Argumentation (0.33 to 0.66), and Detailed Analysis (0.33 to 0.70), with full details available in Table~\ref{tab:radar_chart_capability_scores}. These results indicate stronger alignment with expert-style reasoning. Alignment improvements are also visible in profile scores (Figure~\ref{fig:profiling_summary_comparison} (center)), with intervened outputs matching sophisticated expert types like ``Comprehensive Analyst'' (e.g., Qwen-2.5-VL-7B achieving an alignment score of 0.778 for this profile, as detailed in Table~\ref{tab:profile_alignment_scores}) more closely than baseline.

\begin{table*}[]
\centering
\caption{Top performing model and persona combinations across capability dimensions. Expert Alignment measures the degree to which model outputs match the characteristic patterns of our five expert profiles.}
\label{tab:overall_rankings_summary}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{Configuration} & \textbf{Composite Score} & \textbf{Expert Alignment} \\
\midrule
1 & Qwen-2.5-VL-7B + Mama Zola (佐拉妈妈) + KB & 9.2/10 & 100\% \\
2 & meta-llama_Llama-4-Scout-17B-16E-Instruct + John Ruskin (约翰·罗斯金) + KB & 8.9/10 & 97\% \\
3 & meta-llama_Llama-4-Scout-17B-16E-Instruct + Mama Zola (佐拉妈妈) + KB & 8.7/10 & 95\% \\
4 & meta-llama_Llama-4-Scout-17B-16E-Instruct + Brother Thomas (托马斯修士) + KB & 8.5/10 & 92\% \\
5 & meta-llama_Llama-4-Scout-17B-16E-Instruct + Su Shi (苏轼) + KB & 8.5/10 & 92\% \\
\midrule
-- & Human Expert Benchmark (avg) & 9.2/10 & 100\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Cross-Model Comparison and Configurations}


Qwen-2.5-VL and LLaMA-4-Scout-17B demonstrate strong performance under intervention. In Figure~\ref{fig:profiling_summary_comparison} (left), which visualizes data from Table~\ref{tab:key_feature_scores}, both models demonstrate high scores in areas like Artistic Conception (Qwen: 0.891, Llama-4: 0.851), Brushwork Technique (Qwen: 0.937, Llama-4: 0.903), and Layout and Structure (Qwen: 0.895, Llama-4: 0.916). Their profile alignment in Figure~\ref{fig:profiling_summary_comparison} (center) confirms their ability to emulate multiple expert types. The overall performance rankings, detailed in Table~\ref{tab:overall_rankings_summary}, reveal that the Qwen-2.5-VL-7B model, when guided by the Mama Zola persona and an external knowledge base, achieved the top composite score (9.2/10) and expert alignment (100\%).

The Expert Alignment metric quantifies how closely a model's output matches our five predefined expert profiles (Comprehensive Analyst, Historically Focused Critic, etc.). For each generated commentary, we compute its 47-dimensional feature vector (38 primary features plus 9 derived dimensions) using the zero-shot classification model. We then calculate the cosine similarity between this vector and the centroid vectors of each expert profile, derived from human expert commentaries in MHEB. The commentary is assigned to the profile with highest similarity (threshold $> 0.7$). 

The percentage represents the proportion of outputs successfully matched to an expert profile. A 100\% alignment indicates that all of the model's outputs under that configuration strongly resemble at least one expert archetype, with similarity scores exceeding 0.7. Lower percentages indicate outputs that fall between profiles or lack distinctive expert characteristics. This metric helps assess whether interventions guide models toward recognizable expert-like critique patterns rather than generic responses.

These results show that interpretive capability in VLMs can be substantially improved by structured prompting and domain-specific conditioning. Culturally aligned personas are particularly effective, highlighting the potential of the VULCA framework to guide VLMs toward expert-level reasoning in specialized domains. The distribution of VLM outputs in semantic space, based on their profile scores (centroids detailed in Appendix Table~\ref{tab:reduced_coords_baseline_sources}), also shifts with interventions, indicating changes in their overall analytical posture.

%\section{Discussion}
%\label{sec:discussion}

%This study demonstrates that while baseline VLMs exhibit a notable semantic and capability gap compared to human experts in Chinese art critique, targeted interventions using personas and knowledge bases can significantly improve alignment. The VULCA framework provides a robust methodology for quantifying these changes. Our findings highlight VLMs' potential in specialized domains but also underscore the need for culturally aware prompting and knowledge integration for nuanced understanding. The observed 20-30\% capability enhancement in some models via our interventions is a promising step.

%Key contributions include the VULCA framework itself as a multi-dimensional evaluation tool and the empirical demonstration of intervention effectiveness. This offers pathways for developing more culturally attuned and expert-like VLMs. The critic personas, derived from human expert data, provide a practical mechanism for guiding VLMs towards desired analytical styles.

%Limitations include the specific set of VLMs and artworks; future work could broaden this scope. The definition of "expert critique" is also culturally situated and can be further explored. Investigating more sophisticated knowledge integration techniques and dynamic persona adaptation are promising future directions. Further research could also explore cross-cultural VLM critique capabilities.

\section{Conclusion}
\label{sec:conclusion}

This research introduced VULCA, a quantitative framework for evaluating VLM-generated critiques of traditional Chinese painting. Our experiments demonstrate that persona and knowledge-based interventions significantly enhance VLM performance, achieving closer alignment with human expert standards. The study underscores the importance of culturally grounded approaches for developing VLMs capable of nuanced engagement with specialized domains, paving the way for more sophisticated AI-assisted cultural analysis across diverse contexts.

\section*{Acknowledgments}

We thank the anonymous reviewers for their insightful comments and constructive suggestions that significantly improved this paper. We are grateful to the three annotators with graduate-level training in Chinese art history who contributed to establishing our human expert benchmark dataset, dedicating 15-20 minutes per commentary to ensure high-quality annotations. We acknowledge the Palace Museum Beijing, National Palace Museum Taipei, and the Metropolitan Museum of Art for providing access to their museum catalogs and documentation of Giuseppe Castiglione's ``Twelve Months'' series, which formed the foundation of our expert commentary corpus. We also thank the art history scholars whose peer-reviewed publications and monographs on Qing court painting provided essential domain expertise for this work. %Computational experiments were supported by the University of Dundee's computing infrastructure. Special thanks to the open-source community for maintaining the vLLM framework and the BAAI/bge-large-zh-v1.5 embedding model that were instrumental in our implementation.

We acknowledge the use of AI-powered tools in this research. Claude Code (Anthropic) assisted with code development and debugging throughout the experimental implementation. Claude also provided English language refinement and editorial suggestions during the manuscript preparation. All scientific insights, experimental design, and final editorial decisions remained under full human control and responsibility.

\section*{Limitations}
\label{sec:limitations}

While our VULCA framework demonstrates significant improvements in VLM cultural adaptability, several limitations should be acknowledged. Beyond the specific points enumerated below, this study confronts broader limitations inherent in current AI capabilities and evaluation methodologies. Models, despite interventions, may still reflect biases from their foundational training data or struggle with true generalization to vastly different cultural artifacts or artistic forms beyond the Chinese paintings studied.

\textbf{Dataset and Domain Limitations.} Our evaluation is based on 163 expert commentaries from a single artistic tradition (Qing Dynasty court paintings). We focused exclusively on the ``Twelve Months'' series by Giuseppe Castiglione. Although carefully curated, this dataset may not fully capture the diversity of Chinese art criticism or generalize to other artistic traditions or art forms (calligraphy, sculpture, contemporary art). The annotations on input images may influence VLM outputs in ways that differ from how they would process unannotated images. Cultural nuances may be lost in translation between Chinese and English, particularly for specialized art terminology.

\textbf{Model Selection and Evaluation.} We evaluated a limited set of VLMs due to computational constraints. Newer models or those specifically trained on art history might show different patterns of improvement. Our API-based approach precludes deep analysis of models' internal mechanisms. Despite our standardized approach, VLMs may exhibit sensitivity to minor variations in prompt phrasing or structure, affecting the consistency of results. Our study represents a snapshot of current VLM capabilities, which are rapidly evolving.

\textbf{Methodological Constraints.} Our vector space analysis relies on a specific embedding model (BAAI/bge-large-zh-v1.5), and results might vary with different models. Visualizations using dimensionality reduction techniques (t-SNE, UMAP) inevitably lose some information from the original high-dimensional space. Cosine similarity and other metrics provide useful quantitative comparisons but may not perfectly align with human judgments of semantic similarity in specialized domains. The structured format may artificially constrain both human and VLM expression patterns, potentially reducing stylistic diversity and creative interpretation.

\textbf{Evaluation Subjectivity.} Despite our systematic approach using zero-shot classification and rule-based persona matching, some aspects of art criticism evaluation remain inherently subjective. The choice of feature dimensions and quality metrics reflects particular theoretical perspectives that may not be universally accepted. The template-based section may artificially boost VLM performance by providing explicit categories and prompts that guide responses. Converting existing human expert commentaries to our structured format required interpretation and adaptation, potentially introducing biases.

\textbf{Cultural Complexity.} Art criticism involves tacit knowledge, cultural intuition, and embodied experience that current computational approaches cannot fully capture. Our metrics may miss subtle aspects of genuine cultural understanding versus sophisticated pattern matching. The very tools of our framework, such as the zero-shot classifier for feature extraction and the predefined granularity of persona cards and knowledge bases, introduce their own constraints and potential blind spots. A significant challenge remains in distinguishing between genuine understanding or deep cultural adaptability and sophisticated pattern matching or role-play by the models.



% References section - moved before appendix
\bibliography{references}

\appendix
% \section{示例附录标题} % 您可以将此替换为您的实际附录标题
% \label{sec:example_appendix}
% 这里是附录A的内容。
%
% % 如果您有更多附录，可以继续添加 \section{...}
% % 例如:
% % \section{另一个附录标题}
% % \label{sec:another_appendix}
% % 这里是附录B的内容。

% 您之前注释掉的 \input{appendix} 如果需要，可以取消注释并放在这里，
% 前提是您有一个 appendix.tex 文件。
\input{appendix}

\end{document}
