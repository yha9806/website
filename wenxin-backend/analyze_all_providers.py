"""
åˆ†ææ‰€æœ‰æä¾›å•†çš„æ•°æ®åˆ†å¸ƒ
"""
import json
from pathlib import Path

def analyze_data():
    print("="*60)
    print("æ‰€æœ‰æä¾›å•†æ•°æ®åˆ†æ")
    print("="*60)
    
    # 1. åˆ†æcompleteæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰æä¾›å•†ï¼‰
    complete_file = Path("benchmark_results/complete/complete_benchmark_report.json")
    if complete_file.exists():
        with open(complete_file, 'r', encoding='utf-8') as f:
            complete_data = json.load(f)
        
        print("\nğŸ“ COMPLETEæ–‡ä»¶ï¼ˆæ‰€æœ‰æä¾›å•†æ±‡æ€»ï¼‰:")
        print(f"è·¯å¾„: {complete_file}")
        print(f"æ€»æ¨¡å‹æ•°: {complete_data['models_tested']}")
        print(f"æ€»æµ‹è¯•æ•°: {complete_data['total_tests']}")
        
        # æŒ‰æä¾›å•†åˆ†æ
        provider_results = {}
        for result in complete_data['all_results']:
            model_id = result['model_id']
            
            # åˆ¤æ–­æä¾›å•†
            if 'gpt' in model_id or 'o1' in model_id or 'o3' in model_id:
                provider = 'OpenAI'
            elif 'deepseek' in model_id:
                provider = 'DeepSeek'
            elif 'claude' in model_id:
                provider = 'Anthropic'
            elif 'qwen' in model_id:
                provider = 'Qwen'
            else:
                provider = 'Other'
            
            if provider not in provider_results:
                provider_results[provider] = {
                    'total': 0,
                    'success': 0,
                    'models': set()
                }
            
            provider_results[provider]['total'] += 1
            provider_results[provider]['models'].add(model_id)
            if result.get('success'):
                provider_results[provider]['success'] += 1
        
        print("\næŒ‰æä¾›å•†åˆ†å¸ƒ:")
        for provider, stats in provider_results.items():
            success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
            print(f"\n{provider}:")
            print(f"  - æµ‹è¯•æ•°: {stats['total']}")
            print(f"  - æˆåŠŸæ•°: {stats['success']}")
            print(f"  - æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"  - æ¨¡å‹: {', '.join(sorted(stats['models']))}")
    
    # 2. åˆ†æopenaiæ–‡ä»¶ï¼ˆä¸»æ–‡ä»¶ï¼Œä½†åŒ…å«æ‰€æœ‰æ•°æ®ï¼‰
    openai_file = Path("benchmark_results/openai/openai_benchmark_report.json")
    if openai_file.exists():
        with open(openai_file, 'r', encoding='utf-8') as f:
            openai_data = json.load(f)
        
        print("\n" + "="*60)
        print("ğŸ“ OPENAIæ–‡ä»¶ï¼ˆå®é™…åŒ…å«æ‰€æœ‰æä¾›å•†ï¼‰:")
        print(f"è·¯å¾„: {openai_file}")
        print(f"æ€»æ¨¡å‹æ•°: {openai_data['models_tested']}")
        print(f"æ€»æµ‹è¯•æ•°: {openai_data['total_tests']}")
        
        # æ˜¾ç¤ºæ’åä¸­çš„éOpenAIæ¨¡å‹
        non_openai = []
        for ranking in openai_data['rankings']:
            model_id = ranking['model_id']
            if 'deepseek' in model_id or 'claude' in model_id or 'qwen' in model_id:
                non_openai.append(f"{model_id}: {ranking['average_score']:.1f}")
        
        if non_openai:
            print("\néOpenAIæ¨¡å‹æ’å:")
            for model in non_openai:
                print(f"  - {model}")
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰å•ç‹¬çš„æä¾›å•†æ–‡ä»¶å¤¹
    results_dir = Path("benchmark_results")
    print("\n" + "="*60)
    print("ç›®å½•ç»“æ„:")
    
    for provider_dir in results_dir.iterdir():
        if provider_dir.is_dir():
            files = list(provider_dir.glob("*.json"))
            print(f"\n{provider_dir.name}/")
            for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                print(f"  - {file.name}")
            if len(files) > 5:
                print(f"  ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
    
    # 4. æ•°æ®å®Œæ•´æ€§æ€»ç»“
    print("\n" + "="*60)
    print("æ•°æ®å­˜å‚¨è¯´æ˜:")
    print("-"*60)
    print("1. openai/openai_benchmark_report.json")
    print("   âš ï¸ åç§°æœ‰è¯¯å¯¼æ€§ï¼Œå®é™…åŒ…å«æ‰€æœ‰æä¾›å•†çš„æ•°æ®")
    print("   âœ… åŒ…å«: OpenAI, DeepSeek, Anthropic, Qwenç­‰æ‰€æœ‰æµ‹è¯•ç»“æœ")
    print()
    print("2. complete/complete_benchmark_report.json")
    print("   âœ… å®Œæ•´å¤‡ä»½ï¼Œä¸openaiæ–‡ä»¶å†…å®¹ç›¸åŒ")
    print()
    print("3. å•ä¸ªæä¾›å•†æµ‹è¯•:")
    print("   âŒ æ²¡æœ‰deepseek/, anthropic/, qwen/ç­‰å•ç‹¬æ–‡ä»¶å¤¹")
    print("   åŸå› : run_all_models_benchmark.pyå°†æ‰€æœ‰ç»“æœä¿å­˜åˆ°åŒä¸€æ–‡ä»¶")
    
    print("\n" + "="*60)
    print("å»ºè®®:")
    print("å¦‚éœ€æŒ‰æä¾›å•†åˆ†ç¦»æ•°æ®ï¼Œå¯ä»¥è¿è¡Œ benchmark_by_provider.py")
    print("è¯¥è„šæœ¬ä¼šåˆ›å»ºå•ç‹¬çš„æä¾›å•†æ–‡ä»¶å¤¹å’Œç»“æœæ–‡ä»¶")

if __name__ == "__main__":
    analyze_data()