# AI Model Benchmark Testing Structure

## ğŸ“ Directory Structure

```
benchmark/
â”œâ”€â”€ scripts/                    # æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ openai/                # OpenAI æµ‹è¯•è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ test_openai_all.py
â”‚   â”‚   â””â”€â”€ test_openai_remaining.py
â”‚   â”œâ”€â”€ deepseek/              # DeepSeek æµ‹è¯•è„šæœ¬
â”‚   â”‚   â””â”€â”€ test_deepseek_all.py
â”‚   â”œâ”€â”€ anthropic/             # Anthropic (Claude) æµ‹è¯•è„šæœ¬
â”‚   â”‚   â””â”€â”€ test_anthropic_all.py
â”‚   â”œâ”€â”€ google/                # Google (Gemini) æµ‹è¯•è„šæœ¬
â”‚   â”‚   â””â”€â”€ test_gemini_all.py
â”‚   â”œâ”€â”€ xai/                   # X.AI (Grok) æµ‹è¯•è„šæœ¬
â”‚   â”‚   â””â”€â”€ test_xai_all.py
â”‚   â”œâ”€â”€ moonshot/              # Moonshot (Kimi) æµ‹è¯•è„šæœ¬
â”‚   â”‚   â””â”€â”€ test_moonshot_all.py
â”‚   â””â”€â”€ base_tester.py         # åŸºç¡€æµ‹è¯•ç±»
â”‚
â”œâ”€â”€ results/                    # æµ‹è¯•ç»“æœ
â”‚   â”œâ”€â”€ openai/                # OpenAI ç»“æœ
â”‚   â”‚   â”œâ”€â”€ raw/              # åŸå§‹æµ‹è¯•æ•°æ®
â”‚   â”‚   â”‚   â”œâ”€â”€ o1_results.json
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt-4o_results.json
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ openai_summary.json
â”‚   â”‚   â””â”€â”€ openai_report.md
â”‚   â”œâ”€â”€ deepseek/              # DeepSeek ç»“æœ
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ deepseek_summary.json
â”‚   â”‚   â””â”€â”€ deepseek_report.md
â”‚   â”œâ”€â”€ anthropic/             # Anthropic ç»“æœ
â”‚   â”œâ”€â”€ google/                # Google ç»“æœ
â”‚   â”œâ”€â”€ xai/                   # X.AI ç»“æœ
â”‚   â”œâ”€â”€ moonshot/              # Moonshot ç»“æœ
â”‚   â””â”€â”€ complete/              # å®Œæ•´æ±‡æ€»
â”‚       â”œâ”€â”€ all_models_summary.json
â”‚       â”œâ”€â”€ all_models_rankings.json
â”‚       â””â”€â”€ complete_report.md
â”‚
â””â”€â”€ configs/                    # é…ç½®æ–‡ä»¶
    â”œâ”€â”€ test_cases.json        # æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
    â””â”€â”€ scoring_prompt.txt     # è¯„åˆ†æç¤ºæ¨¡æ¿
```

## ğŸ¯ Testing Standards

### 1. Test Cases (3 standard tests)
- **poem_moon**: Poetry creation about moon and stars
- **story_robot**: Short story about robot learning to paint  
- **code_fibonacci**: Python Fibonacci sequence function

### 2. Scoring Dimensions (0-100 each)
- **Rhythm**: Flow and pacing
- **Composition**: Structure and organization
- **Narrative**: Storytelling ability
- **Emotion**: Emotional expression
- **Creativity**: Originality and imagination
- **Cultural**: Cultural relevance

### 3. Scoring Method
- All models scored by GPT-4o-mini
- Temperature: 0.3 for consistency
- JSON response format enforced
- NO FALLBACK scoring allowed

## ğŸ“Š Data Structure

### Raw Result Format
```json
{
  "model_id": "gpt-4o",
  "test_id": "poem_moon",
  "success": true,
  "timestamp": "2025-08-19T12:00:00",
  "duration": 5.2,
  "response": "model response text...",
  "response_length": 1234,
  "score_details": {
    "total_score": 87,
    "dimensions": {
      "rhythm": 85,
      "composition": 90,
      "narrative": 82,
      "emotion": 88,
      "creativity": 86,
      "cultural": 91
    },
    "highlights": ["point 1", "point 2"],
    "weaknesses": ["point 1"]
  }
}
```

### Summary Format
```json
{
  "provider": "OpenAI",
  "test_date": "2025-08-19",
  "models_tested": 12,
  "successful_tests": 30,
  "failed_tests": 6,
  "rankings": [
    {
      "rank": 1,
      "model_id": "o1",
      "average_score": 88.3,
      "tests_completed": 3
    }
  ]
}
```

## ğŸš€ Usage

### Test single provider
```bash
python benchmark/scripts/openai/test_openai_remaining.py
```

### Test all providers
```bash
python benchmark/scripts/run_all_providers.py
```

### Generate reports
```bash
python benchmark/scripts/generate_reports.py
```

## ğŸ“ˆ Current Status

| Provider | Models | Tested | Success | Average Score |
|----------|--------|--------|---------|---------------|
| OpenAI | 12 | 9 | 8 | 82.1 |
| DeepSeek | 3 | 1 | 1 | 82.0 |
| Anthropic | 3 | 0 | 0 | - |
| Google | 1 | 0 | 0 | - |
| X.AI | 2 | 0 | 0 | - |
| Moonshot | 2 | 0 | 0 | - |