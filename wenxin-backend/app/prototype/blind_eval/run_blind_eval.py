#!/usr/bin/env python3
"""CLI entry point for blind evaluation experiment.

Usage:
    # Mock mode (validate structure, $0)
    python3 app/prototype/blind_eval/run_blind_eval.py --mode mock

    # Real mode with diffusers
    python3 app/prototype/blind_eval/run_blind_eval.py --mode real --provider diffusers

    # Export blinded materials only (requires completed run)
    python3 app/prototype/blind_eval/run_blind_eval.py --export-only

    # Analyze completed annotations
    python3 app/prototype/blind_eval/run_blind_eval.py --analyze-only \
        --e1-annotations path/to/e1.csv --e2-annotations path/to/e2.csv
"""

from __future__ import annotations

import argparse
import logging
import sys
from pathlib import Path

# Ensure project root is on path
_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(_ROOT))

from app.prototype.blind_eval.experiment_config import build_default_config
from app.prototype.blind_eval.task_sampler import sample_tasks
from app.prototype.blind_eval.experiment_runner import ExperimentRunner
from app.prototype.blind_eval.blind_exporter import BlindExporter
from app.prototype.blind_eval.blind_analyzer import generate_report

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger("blind_eval")

_RESULTS_DIR = Path(__file__).resolve().parent / "results"


def main() -> None:
    parser = argparse.ArgumentParser(description="VULCA Blind Evaluation Experiment")
    parser.add_argument(
        "--mode", choices=["mock", "real"], default="mock",
        help="Execution mode: mock ($0) or real (uses providers)",
    )
    parser.add_argument(
        "--provider", default="mock",
        help="Provider for real mode (default: mock)",
    )
    parser.add_argument(
        "--include-ablation", action="store_true",
        help="Include ablation group (3 groups instead of 2)",
    )
    parser.add_argument(
        "--export-only", action="store_true",
        help="Only export blinded materials from existing results",
    )
    parser.add_argument(
        "--analyze-only", action="store_true",
        help="Only analyze completed annotations",
    )
    parser.add_argument(
        "--e1-annotations", type=Path, default=None,
        help="Path to E1 annotation CSV",
    )
    parser.add_argument(
        "--e2-annotations", type=Path, default=None,
        help="Path to E2 annotation CSV",
    )
    parser.add_argument(
        "--results-dir", type=Path, default=_RESULTS_DIR,
        help="Results directory",
    )
    parser.add_argument(
        "--tasks-subset", type=int, default=None,
        help="Limit tasks per category (for quick testing)",
    )

    args = parser.parse_args()

    if args.analyze_only:
        logger.info("=== Analyze Only Mode ===")
        report = generate_report(
            args.results_dir,
            e1_annotations=args.e1_annotations,
            e2_annotations=args.e2_annotations,
        )
        print(report)
        return

    # Sample tasks
    n_per_cat = args.tasks_subset or 10
    tasks = sample_tasks(n_per_category=n_per_cat)
    logger.info("Sampled %d tasks (%d per category)", len(tasks), n_per_cat)

    # Build config
    provider = args.provider if args.mode == "real" else "mock"
    config = build_default_config(
        tasks=tasks,
        provider=provider,
        include_ablation=args.include_ablation,
    )

    if args.export_only:
        logger.info("=== Export Only Mode ===")
        exporter = BlindExporter(args.results_dir, config)
        e1_dir, e2_dir = exporter.export_all()
        logger.info("E1 exported to: %s", e1_dir)
        logger.info("E2 exported to: %s", e2_dir)
        return

    # Run experiment
    logger.info("=== Running Blind Evaluation (%s mode) ===", args.mode)
    logger.info("Groups: %s", [g.name for g in config.groups])
    logger.info("Provider: %s", provider)

    runner = ExperimentRunner(config, results_dir=args.results_dir)
    all_results = runner.run_all(tasks)

    # Summary
    for group_name, results in all_results.items():
        successes = sum(1 for _, o, _ in results if o.success)
        logger.info("  %s: %d/%d success", group_name, successes, len(results))

    # Export blinded materials
    logger.info("=== Exporting Blinded Materials ===")
    exporter = BlindExporter(args.results_dir, config)
    e1_dir, e2_dir = exporter.export_all()
    logger.info("E1 exported to: %s", e1_dir)
    logger.info("E2 exported to: %s", e2_dir)

    # Generate preliminary analysis (no annotations yet)
    report = generate_report(args.results_dir)
    logger.info("Analysis report saved to: %s", args.results_dir / "analysis")

    logger.info("=== Blind Evaluation Complete ===")
    logger.info("Next steps:")
    logger.info("  1. Distribute E1/E2 materials to raters")
    logger.info("  2. Collect filled annotation CSVs")
    logger.info("  3. Run: python3 ... --analyze-only --e1-annotations ... --e2-annotations ...")


if __name__ == "__main__":
    main()
